{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stochastic_Processes_&_Optimization_in_Machine_Learning_(Lab_6_Markov_Decision_Processes).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLZdEbAy2jfr"
      },
      "source": [
        "<h1><b>Markov Decision Processes</h1></b>\n",
        "<p align=\"justify\">Στη συγκεκριμένη άσκηση θα μελετήσετε τους αλγορίθμους <i>Policy Iteration</i> και <i>Value Iteration</i>, καθώς και θα εξοικειωθείτε με βασικές έννοιες των <i>Markov Decision Processes</i>. Οι αλγόριθμοι <i>Policy Iteration</i> και <i>Value Iteration</i> είναι από τους βασικούς αλγορίθμους δυναμικού προγραμματισμού που χρησιμοποιούνται για την επίλυση της εξίσωσης <i>Bellman</i> σε <i>Markov Decision Processes</i>.</p> \n",
        "<p align=\"justify\">Το πρόβλημα που θα μελετήσετε είναι αυτό της παγωμένης λίμνης (Frozen Lake) με μέγεθος πλέγματος 8 x 8.</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VsUV229__zO"
      },
      "source": [
        "<h2><b>Εξοικείωση με τη βιβλιοθήκη <i>Gym</i></b></h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OM8ivgOJAg_H"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import wrappers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_puV3ugeAnbU"
      },
      "source": [
        "Με την παρακάτω εντολή, ορίζετε το πρόβλημα που θα μελετηθεί:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ep-MvIUCAxT8"
      },
      "source": [
        "env_name = 'FrozenLake8x8-v0'\n",
        "env = gym.make(env_name)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBKBXJDUBRUh"
      },
      "source": [
        "Με τις παρακάτω εντολές, θα επαναφέρετε τον Agent στην αρχική του θέση και θα οπτικοποιήσετε το πλέγμα και τη θέση του Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6lqbG9zBgdi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0a3e4c7-6dc6-4fa9-f70b-ef89aaf28331"
      },
      "source": [
        "env.reset()\n",
        "env.render()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FX2res4JBlYb"
      },
      "source": [
        "Με τις παρακάτω εντολές, ορίζετε την επόμενη ενέργεια με τυχαίο τρόπο και ο Agent κάνει ένα βήμα."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq7q944YBx0Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17c3a97e-9618-492a-c6b3-89eb2fc0d646"
      },
      "source": [
        "next_action = env.action_space.sample()\n",
        "env.step(next_action)\n",
        "env.render()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (Down)\n",
            "SFFFFFFF\n",
            "F\u001b[41mF\u001b[0mFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV4A7lsLB54y"
      },
      "source": [
        "Να εκτελέσετε αρκετές φορές τις τελευταίες εντολές και να παρατηρήσετε κάθε φορά την ενέργεια που ζητείται από τον agent να εκτελέσει και την ενέργεια που αυτός πραγματοποιεί. Πραγματοποιεί πάντοτε ο agent την κίνηση που του ζητείται; Είναι ντετερμινιστικές ή στοχαστικές οι κινήσεις του agent;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38AT6MOSQBQ6"
      },
      "source": [
        "Ο agent δεν πραγματοποιεί πάντοτε την κίνηση που ζητείτε και αυτό οφείλεται στον στοχαστικό χαρακτήρα των κινήσεων."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAL4we3gDV_J"
      },
      "source": [
        "<h2><b>Ερωτήσεις</b></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQKm4VAUChi1"
      },
      "source": [
        "<ul>\n",
        "<li>Μελετώντας <a href=\"https://gym.openai.com/envs/FrozenLake-v0/\">αυτό</a> και βασισμένοι στα συμπεράσματα του προηγούμενου ερωτήματος να περιγράψετε σύντομα το πρόβλημα της παγωμένης λίμνης ως πρόβλημα βελτιστοποίησης. Ποιος είναι ο στόχος του agent;</li>\n",
        "<li>Να διατυπώσετε την ιδιότητα <i>Markov</i>. Πώς απλοποιεί η ιδιότητα αυτή τη μελέτη του συγκεκριμένου προβλήματος;</li>\n",
        "<li>Να περιγράψετε σύντομα τους αλγορίθμους <i>Policy Iteration</i> και <i>Value Iteration</i>, δίνοντας έμφαση στο διαφορετικό τρόπο με τον οποίο προσεγγίζουν την επίλυση του προβλήματος. Είναι εγγυημένο ότι οι δύο αλγόριθμοι θα συγκλίνουν στη βέλτιστη πολιτική; Αν ναι, οδηγούν σε ίδια ή διαφορετική βέλτιστη πολιτική;</li>\n",
        "<li>Να εκτελέσετε τα προγράμματα που σας δίνονται, τα οποία επιλύουν το\n",
        "πρόβλημα της παγωμένης λίμνης, χρησιμοποιώντας τους αλγορίθμους <i>Policy\n",
        "Iteration</i> και <i>Value Iteration</i> αντίστοιχα. Ποια μέθοδος συγκλίνει στη βέλτιστη λύση σε λιγότερα βήματα; Τι συμπέρασμα βγάζετε; Να ελέγξετε το χρόνο εκτέλεσης του κάθε προγράμματος, χρησιμοποιώντας την εντολή <i>time</i>. Τι συμπέρασμα βγάζετε ως προς την πολυπλοκότητα του κάθε αλγορίθμου;</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4n5JPREQReRB"
      },
      "source": [
        "Απαντήσεις :\n",
        "\n",
        "\n",
        "\n",
        "*   Το πρόβλημα της παγωμένης λίμνης είναι το εξής : Προσπαθείς να πας από το αρχικό σημείο στον στόχο σου και πρέπει να διασχίσεις μια παγωμένη λίμνη. Η παγωμένη λίμη έχει σημεία που έχει λιώσει ο πάγος και πέφτεις μέσα στη τρύπα. Όταν μοντελοποιηθεί το πρόβλημα δίνουμε +1 σαν ανταμοιβή αν φτάσουμε στον στόχο και -1 ανταμοιβή αν πέσουμε σε τρύπα και 0 στις υπόλοιπες καταστάσεις. Η παγωμένη λίμνη μεταφράζεται σαν πρόβλημα βελτιστοποίησης όταν θέλουμε να βρούμε τις κινήσεις , δηλαδή την πολιτική, που θα μας οδηγήσουν στην μέγιστη ανταμοιβή.\n",
        "\n",
        "*    H ιδιότητα του Markov είναι ότι κάθε κατάσταση δεν εξαρτάται από το παρελθόν της. Αυτή η ιδιότητα βοηθάει να απλοποιηθεί το πρόβλημα καθώς πλέον η πολιτική ειναι χρονοσταθερή (stationary).\n",
        "\n",
        "* Ο γενικός στόχος των δύο αυτών αλγορίθμων είναι να προσφέρουν στον agent την βέλτιστη πολιτική για κάθε κατάσταση, δηλαδή την κίνηση που θα πρέπει να εκτελέσει σε κάθε θέση για να έχουμε τελικό την μέγιστη. Αν έχει στοχαστική φύση την κίνhση που θα του δώσουμε θα την εκτελέσει με μια πιθανότητα.\n",
        "\n",
        "* Δύο έννοιες που πρέπει να εισάγουμε είναι το V function και το Q function. To V function διαισθητικά δίνει μια τιμή σε κάθε κατάσταση και προσδιορίζει το πόσο καλή ειναι η συγκεκριμένη κατάσταση και την αναμενόμενη ανταμοιβή που θα έχει κάποιος αν βρεθεί σε αυτή τη θέση. Το Q function προσδιορίζει το πόσο καλή είναι μια συγκεκριμένη κίνηση σε κάθε κατάσταση. Το V function κάθε κατάστασης υπολογίζεται ως το μέγιστο Q function στο σύνολο των κινήσεων για μια κατάσταση.\n",
        "\n",
        "* Συνεπώς μπορούμε να εξηγήσουμε συνποτικα τον Value Iteration. O οποίος σε κάθε επανάληψη κοιτάει να ανανεώσει τις τιμές των V functions σε κάθε κατάσταση.Όταν πλέον έχουμε σύγκλιση και δεν αλλάζουν οι τιμές αυτές μπορούμε εύκολα να βρούμε την πολιτική που προσφέρεται ακολουθώντας τις κινήσεις που προσφέρουν την μέγιστη V function τιμή , δηλαδή ενα greedy κριτήριο.\n",
        "\n",
        "* Ο αλγόριθμος Policy Iteration δεν κοιτάει να ανανεώσειει τα V functions άμεσα, αλλά κύριος του στόχος είναι να βελτιωθεί η πολιτική που ακολουθεί. Αυτο επιτυγχάνεται με το να υπολογίζει σε κάθε επανάληψη τα V function και να τα χρησιμοποιεί για να προκύπτει μια πολιτική. Ύστερα εκμεταλλέυεται την ανανεωμένη πολιτική και υπολογίζει εκ νέου τα V functions που θα οδηγήσουν σε νέα πολιτική μέχρι να μην έχουμε αλλαγή στη πολιτική.\n",
        "\n",
        "\n",
        "*   Και οι δύο αλγόριθμοι θα οδηγήσουν σε βέλτιστη πολιτική αλλά δεν ειναι εγγυημένο ότι θα είναι η ίδια. Απο την διαφορετική προσέγγιση τους ενδέχεται να καταλήξουν σε διαφορετικές πολιτικές που όμως θα είναι ισοδύναμες ως προς την ανταμοιβή.\n",
        "\n",
        "\n",
        "\n",
        "*   Εκτελούμε τους 2 αλγορίθμους και βλέπουμε ότι ο Policy Iteration συγκλίνει σε 6 βήματα και 2 second στην πολιτική του. Ο Value Iteration συγκλίνει σε 2357 βήματα και 1 second στην πολιτική του. Από τα παραπάνω παρατηρούμε οτι ο Policy Iteration έχει πολύ μεγαλύτερη χρονική πολυπλοκότητα από ότι ο Value Iteration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6mci5P4HJ_1"
      },
      "source": [
        "<h2><b>Policy Iteration</b></h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_43MjfJ9G8v7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d947405-9332-4776-d9ce-9a79b2693d40"
      },
      "source": [
        "\"\"\"\n",
        "Solving FrozenLake8x8 environment using Policy iteration.\n",
        "Author : Moustafa Alzantot (malzantot@ucla.edu)\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym import wrappers\n",
        "\n",
        "\n",
        "def run_episode(env, policy, gamma = 1.0, render = False):\n",
        "    \"\"\" Runs an episode and return the total reward \"\"\"\n",
        "    obs = env.reset()\n",
        "    total_reward = 0\n",
        "    step_idx = 0\n",
        "    while True:\n",
        "        if render:\n",
        "            env.render()\n",
        "        obs, reward, done , _ = env.step(int(policy[obs]))\n",
        "        total_reward += (gamma ** step_idx * reward)\n",
        "        step_idx += 1\n",
        "        if done:\n",
        "            break\n",
        "    return total_reward\n",
        "\n",
        "\n",
        "def evaluate_policy(env, policy, gamma = 1.0, n = 100):\n",
        "    scores = [run_episode(env, policy, gamma, False) for _ in range(n)]\n",
        "    return np.mean(scores)\n",
        "\n",
        "def extract_policy(v, gamma = 1.0):\n",
        "    \"\"\" Extract the policy given a value-function \"\"\"\n",
        "    policy = np.zeros(env.nS)\n",
        "    for s in range(env.nS):\n",
        "        q_sa = np.zeros(env.nA)\n",
        "        for a in range(env.nA):\n",
        "            q_sa[a] = sum([p * (r + gamma * v[s_]) for p, s_, r, _ in  env.P[s][a]])\n",
        "        policy[s] = np.argmax(q_sa)\n",
        "    return policy\n",
        "\n",
        "def compute_policy_v(env, policy, gamma=1.0):\n",
        "    \"\"\" Iteratively evaluate the value-function under policy.\n",
        "    Alternatively, we could formulate a set of linear equations in iterms of v[s] \n",
        "    and solve them to find the value function.\n",
        "    \"\"\"\n",
        "    v = np.zeros(env.nS)\n",
        "    eps = 1e-10\n",
        "    while True:\n",
        "        prev_v = np.copy(v)\n",
        "        for s in range(env.nS):\n",
        "            policy_a = policy[s]\n",
        "            v[s] = sum([p * (r + gamma * prev_v[s_]) for p, s_, r, _ in env.P[s][policy_a]])\n",
        "        if (np.sum((np.fabs(prev_v - v))) <= eps):\n",
        "            # value converged\n",
        "            break\n",
        "    return v\n",
        "\n",
        "def policy_iteration(env, gamma = 1.0):\n",
        "    \"\"\" Policy-Iteration algorithm \"\"\"\n",
        "    policy = np.random.choice(env.nA, size=(env.nS))  # initialize a random policy\n",
        "    max_iterations = 200000\n",
        "    gamma = 1.0\n",
        "    for i in range(max_iterations):\n",
        "        old_policy_v = compute_policy_v(env, policy, gamma)\n",
        "        new_policy = extract_policy(old_policy_v, gamma)\n",
        "        if (np.all(policy == new_policy)):\n",
        "            print ('Policy-Iteration converged at step %d.' %(i+1))\n",
        "            break\n",
        "        policy = new_policy\n",
        "    return policy\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    env_name  = 'FrozenLake8x8-v0'\n",
        "    env = gym.make(env_name)\n",
        "    env = env.unwrapped\n",
        "    optimal_policy = policy_iteration(env, gamma = 1.0)\n",
        "    scores = evaluate_policy(env, optimal_policy, gamma = 1.0)\n",
        "    print('Average scores = ', np.mean(scores))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Policy-Iteration converged at step 6.\n",
            "Average scores =  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcikBq6BHRQM"
      },
      "source": [
        "<h2><b>Value Iteration</b></h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHvcnTDcHGmH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c58da18c-5751-4646-a87e-5c4e90d4f9be"
      },
      "source": [
        "\"\"\"\n",
        "Solving FrozenLake8x8 environment using Value-Itertion.\n",
        "Author : Moustafa Alzantot (malzantot@ucla.edu)\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym import wrappers\n",
        "\n",
        "\n",
        "def run_episode(env, policy, gamma = 1.0, render = False):\n",
        "    \"\"\" Evaluates policy by using it to run an episode and finding its\n",
        "    total reward.\n",
        "    args:\n",
        "    env: gym environment.\n",
        "    policy: the policy to be used.\n",
        "    gamma: discount factor.\n",
        "    render: boolean to turn rendering on/off.\n",
        "    returns:\n",
        "    total reward: real value of the total reward recieved by agent under policy.\n",
        "    \"\"\"\n",
        "    obs = env.reset()\n",
        "    total_reward = 0\n",
        "    step_idx = 0\n",
        "    while True:\n",
        "        if render:\n",
        "            env.render()\n",
        "        obs, reward, done , _ = env.step(int(policy[obs]))\n",
        "        total_reward += (gamma ** step_idx * reward)\n",
        "        step_idx += 1\n",
        "        if done:\n",
        "            break\n",
        "    return total_reward\n",
        "\n",
        "\n",
        "def evaluate_policy(env, policy, gamma = 1.0,  n = 100):\n",
        "    \"\"\" Evaluates a policy by running it n times.\n",
        "    returns:\n",
        "    average total reward\n",
        "    \"\"\"\n",
        "    scores = [\n",
        "            run_episode(env, policy, gamma = gamma, render = False)\n",
        "            for _ in range(n)]\n",
        "    return np.mean(scores)\n",
        "\n",
        "def extract_policy(v, gamma = 1.0):\n",
        "    \"\"\" Extract the policy given a value-function \"\"\"\n",
        "    policy = np.zeros(env.nS)\n",
        "    for s in range(env.nS):\n",
        "        q_sa = np.zeros(env.action_space.n)\n",
        "        for a in range(env.action_space.n):\n",
        "            for next_sr in env.P[s][a]:\n",
        "                # next_sr is a tuple of (probability, next state, reward, done)\n",
        "                p, s_, r, _ = next_sr\n",
        "                q_sa[a] += (p * (r + gamma * v[s_]))\n",
        "        policy[s] = np.argmax(q_sa)\n",
        "    return policy\n",
        "\n",
        "\n",
        "def value_iteration(env, gamma = 1.0):\n",
        "    \"\"\" Value-iteration algorithm \"\"\"\n",
        "    v = np.zeros(env.nS)  # initialize value-function\n",
        "    max_iterations = 100000\n",
        "    eps = 1e-20\n",
        "    for i in range(max_iterations):\n",
        "        prev_v = np.copy(v)\n",
        "        for s in range(env.nS):\n",
        "            q_sa = [sum([p*(r + prev_v[s_]) for p, s_, r, _ in env.P[s][a]]) for a in range(env.nA)] \n",
        "            v[s] = max(q_sa)\n",
        "        if (np.sum(np.fabs(prev_v - v)) <= eps):\n",
        "            print ('Value-iteration converged at iteration# %d.' %(i+1))\n",
        "            break\n",
        "    return v\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    env_name  = 'FrozenLake8x8-v0'\n",
        "    gamma = 1.0\n",
        "    env = gym.make(env_name)\n",
        "    env = env.unwrapped\n",
        "    optimal_v = value_iteration(env, gamma);\n",
        "    policy = extract_policy(optimal_v, gamma)\n",
        "    policy_score = evaluate_policy(env, policy, gamma, n=1000)\n",
        "    print('Policy average score = ', policy_score)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Value-iteration converged at iteration# 2357.\n",
            "Policy average score =  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stochastic_Processes_&_Optimization_in_Machine_Learning_(Lab_6_Q_Learning_2021).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHFOjD_VY0ld"
      },
      "source": [
        "<h1>Q-Learning</h1>\n",
        "\n",
        "<p>Στην συγκεκριμένη άσκηση θα μελετήσετε το στοχαστικό αλγόριθμο Q-Learning, χρησιμοποιώντας το έτοιμο πρόγραμμα που σας δίνεται.</p> <p> Στα πλαίσια του παραδείίγματος θα εξετάσετε μίία υλοποίηση με q-learning σχετικά με το σύστημα αυτόματης οδήγησης ενός ταξί. Στα πλαίσια του προβλήματος αυτού θα πρέπει να ικανοποιούνται τα εξής:</p>\n",
        "<ul>\n",
        "<li>Το ταξί θα πρέπει να αφήνει τον πελάτη στη σωστή θέση</li>\n",
        "<li>Το ταξί να ακολουθεί τη συντομότερη δυνατή διαδρομή</li>\n",
        "<li>Να τηρούνται οι κανόόνες κυκλοφορίας και ασφάλειας των επιβατών</li>\n",
        "</ul>\n",
        "\n",
        "<p>Στα πλαίσια του προβλήματος έχουμε τα εξής χαρακτηριστικά για την επιβράβευση, τις καταστάσεις και τις ενέργειες.</p> \n",
        "\n",
        "<h4>Επιβράβευση</h4>\n",
        "<ul>\n",
        "<li>Θα έχουμε τη μέγιστη επιβράβευση όταν το ταξί αφήνει έναν πελάτη στην επιθυμητή θέση</li>\n",
        "<li>Θα υπάρχει ποινή στην περίπτωση όπου το ταξί αφήσει τον πελάτη σε κάποιο λανθασμένο σημείο</li>\n",
        "<li>Ο agent θα παίρνει μία μικρή σχετικά ποινή στην περίπτωση όπου αργεί να φτάσει στον τελικό προορισμό</li>\n",
        "</ul>\n",
        "\n",
        "<p>Γενικά οι παραπάνω αρχές συνοψίζονται στα εξής: \"Λαμβάνουμε +20 πόντους για μια επιτυχημένη πτώση και χάνουμε 1 πόντο για κάθε χρονικό βήμα που παίρνει. Υπάρχει επίσης ποινή 10 πόντων για παράνομες ενέργειες παραλαβής και αποχώρησης.\"</p>\n",
        "\n",
        "<h4>Πλήθος Καταστάσεων</h4>\n",
        "<img src=\"https://storage.googleapis.com/lds-media/images/Reinforcement_Learning_Taxi_Env.width-1200.png\">\n",
        "<p>Στο παράδειγμά μας έχουμε ένα μικρό χώρο 5*5. Από εκεί και πέρα έχουμε 4 προορισμούς και 5 πιθανές θέσεις του πελάτη (παίρνουμε και την περίπτωση ο πελάτης να είναι ήδη μέσα στο τάξι).</p>\n",
        "<p>Με βάση τα παραπάνω έχουμε 5*5*5*4=500 πιθανές καταστάσεις.</p>\n",
        "\n",
        "<h4>Πλήθος Ενεργειών</h4>\n",
        "<p>Έχουμε έξι ενεργειες για το ταξί, οι οποίες είναι οι εξής:</p>\n",
        "<ul>\n",
        "<li>0=Νότια</li>\n",
        "<li>1=Βόρεια</li>\n",
        "<li>2=Ανατολικά</li>\n",
        "<li>3=Δυτικά</li>\n",
        "<li>4=Επιβίβαση</li>\n",
        "<li>5=Αποβίβαση</li>\n",
        "</ul>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOQpQ0DygDzt"
      },
      "source": [
        "<p>Πριν συνεχίσετε στην άσκηση να απαντήσετε στο εξής ερώτημα:</p>\n",
        "\n",
        "<b><p>1. Να περιγράψετε σύντομα τον αλγόριθμο Q-Learning. Σε ποια προβλήματα θεωρείτε ότι ταιριάζει ως τρόπος εκμάθησης η Ενισχυτική Μάθηση (Reinforcement Learning); Ποια είναι η βασική διαφορά του αλγορίθμου Q-Learning από τους αλγορίθμους Policy Iteration και Value Iteration;</p></b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcxwrqZ7sHaC"
      },
      "source": [
        "* Ο αλγόριθμος Q learning βασίζεται στις τιμές των Q functions. Η τιμή αυτή δίνει για κάθε κατάσταση το αναμενόμενο κέρδος αν ακολουθήσουμε μια συγκεκριμένη κίνηση. 0 αλγόριθμος αυτός αποθηκεύει σε έναν πολυδιάστατο πίνακα τις τιμές των Q functions για όλες τις υποψήφιες καταστάσεις. Σε κάθε επανάληψη ακολουθεί την κίνηση που έχει το μέγιστο Q function και έχουμε ανανέωση της τιμής του Q function. Ο αλγόριθμος σε κάθε αρχικοποίηση του περιβάλλοντος έχει γνώση από το προηγούμενο επεισόδιο και με την ιδιότητα του Temporal Difference υπολογίζει εκ νέου τις Q function και η ανανέωση γίνεται με την διαφορά της νέας με την παλιά τιμή. Ύστερα απο πολλά επεισόδια πρέπει να παρατηρήσουμε σύγκλιση των τιμών.\n",
        "\n",
        "* Η ενισχυτική μαθηση ταιριάζει σε προβήματα βελτιστοποίησης που προκύπτουν από μια ακολουθία ενεργειών\n",
        "\n",
        "* Η βασική διαφορά του αλγορίθμου Q learning και Policy/Value Iteration είναι ότι δεν γνωρίζουμε εξ αρχής τις πιθανότητες μετάβασης και τις ανταμοιβές όπως επίσης o Q learning αλγόριθμος είναι  off policy σε αντίθεση με τους άλλους 2 που είναι on policy. Δηλαδή έχουμε τυχαία αρχικοποιήση θέσης και εξερευνούμε νέες πολιτικές ενώ σε αντίθεση με τους Policy/Value Iteration που έχουμε συνεχώς βελτίωση της αρχικής πολιτικής."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc11bCDZgfSx"
      },
      "source": [
        "<p>Στη συνέχεια θα πρέπει να φορτώσετε τη βιβλιοθήκη gym καθώς και το σχετικό dataset<p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_95lIAzWYpp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "872ec7db-ce91-4e51-afc8-74e2f01d8cdb"
      },
      "source": [
        "!pip install cmake 'gym[atari]' scipy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cmake in /usr/local/lib/python3.7/dist-packages (3.12.0)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.19.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.3.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (0.2.6)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (7.1.2)\n",
            "Requirement already satisfied: opencv-python; extra == \"atari\" in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (4.1.2.30)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0; extra == \"atari\"->gym[atari]) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDHLb5PGWdwr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "789c1181-4a5e-45f4-f4eb-f30f9b9c2a91"
      },
      "source": [
        "import gym\n",
        "\n",
        "env = gym.make(\"Taxi-v3\").env\n",
        "\n",
        "env.render()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R:\u001b[43m \u001b[0m| : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "396kTtOrW-cO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b949fd4-2ea5-4f16-a929-4b551b943071"
      },
      "source": [
        "env.reset() # reset environment to a new, random state\n",
        "env.render()\n",
        "\n",
        "print(\"Action Space {}\".format(env.action_space))\n",
        "print(\"State Space {}\".format(env.observation_space))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "|\u001b[43m \u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "\n",
            "Action Space Discrete(6)\n",
            "State Space Discrete(500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UC6-XuIhF5_"
      },
      "source": [
        "<p>Παρακάτω ορίζουμε τις συνεταγμένες του ταξί, τη θέση του πελάτη και το σημείο προορισμού</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPSOw5CdXFx1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66831f6b-d6f7-47c9-8d5f-ef586c631038"
      },
      "source": [
        "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
        "print(\"State:\", state)\n",
        "\n",
        "env.s = state\n",
        "env.render()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "State: 328\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| |\u001b[43m \u001b[0m: | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsAkQJhchVFy"
      },
      "source": [
        "<p>Παρακάτω είναι η μήτρα επιβράβευσης για το state που ορίσαμε στο προηγούμενο βήμα</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7oDbznIXOJo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa347992-ffa8-47ce-9722-bb043b468597"
      },
      "source": [
        "env.P[328]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: [(1.0, 428, -1, False)],\n",
              " 1: [(1.0, 228, -1, False)],\n",
              " 2: [(1.0, 348, -1, False)],\n",
              " 3: [(1.0, 328, -1, False)],\n",
              " 4: [(1.0, 328, -10, False)],\n",
              " 5: [(1.0, 328, -10, False)]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtROeD1ph6kk"
      },
      "source": [
        "<p> Τρέχουμε το παράδειγμά μας χωρις τη χρήση Q-Learning.</p>\n",
        "\n",
        "<b><p>2. Τα αποτελέσματα είναι ικανοποιητικά; Πως θα μας εξυπηρετούσε η χρήση του Q-Learning;</p></b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKHhcDxVXUFz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69685a5a-5685-46cb-bef2-bebf4691f561"
      },
      "source": [
        "env.s = 328  # set environment to illustration's state\n",
        "\n",
        "epochs = 0\n",
        "penalties, reward = 0, 0\n",
        "\n",
        "frames = [] # for animation\n",
        "\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    action = env.action_space.sample()\n",
        "    state, reward, done, info = env.step(action)\n",
        "\n",
        "    if reward == -10:\n",
        "        penalties += 1\n",
        "    \n",
        "    # Put each rendered frame into dict for animation\n",
        "    frames.append({\n",
        "        'frame': env.render(mode='ansi'),\n",
        "        'state': state,\n",
        "        'action': action,\n",
        "        'reward': reward\n",
        "        }\n",
        "    )\n",
        "\n",
        "    epochs += 1\n",
        "    \n",
        "    \n",
        "print(\"Timesteps taken: {}\".format(epochs))\n",
        "print(\"Penalties incurred: {}\".format(penalties))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Timesteps taken: 3501\n",
            "Penalties incurred: 1160\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFMr4UzO1m1d"
      },
      "source": [
        "Παρατηρούμε ότι χρειάστηκαν πολλά timesteps και έχουμε πολλά penalties, συνεπώς δεν είναι ικανοποιητικά τα αποτελέσματα. Αυτό μπορεί να αποφευχθεί με την χρήση του reinforcement learning το οποίο μπορεί να παρέχει την γνώση από προηγούμενα επεισόδια."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj3s09rsizVm"
      },
      "source": [
        "<p>Τώρα θα προσπαθήσουμε να λύσουμε το πρόβλημά μας με τη χρήση του Q-Learning.</p>\n",
        "\n",
        "<b><p>3. Τι γνωρίζετε για τις παραμέτρους α και γ. Τι θα συμβεί αν έχουν τιμές ίσες με 1;</p></b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7UVO7xR5A2l"
      },
      "source": [
        "* H παράμετρος α καθορίζει την συμβολή της προηγούμενης τιμής του Q function στην νέα τιμή. Αν πάρει την τιμή 1 τότε δεν υπολογίζεται καθόλου στην ανανέωση\n",
        "\n",
        "* Η παράμετρος γ καθορίζει την συμβολή της μελλοντικής Q function, δηλαδή της επόμενης κίνησης. Όταν παίρνει την τιμή 1 τότε έχουμε την πλήρη συμβολή της."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJk3NTfcXrrA"
      },
      "source": [
        "import numpy as np\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy2Yg8DTXtHW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7ab3e6c-7948-4ca3-940d-5177f4ba5375"
      },
      "source": [
        "%%time\n",
        "\"\"\"Training the agent\"\"\"\n",
        "\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.1\n",
        "gamma = 0.6\n",
        "epsilon = 0.1\n",
        "\n",
        "# For plotting metrics\n",
        "all_epochs = []\n",
        "all_penalties = []\n",
        "\n",
        "for i in range(1, 100001):\n",
        "    state = env.reset()\n",
        "\n",
        "    epochs, penalties, reward, = 0, 0, 0\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample() # Explore action space\n",
        "        else:\n",
        "            action = np.argmax(q_table[state]) # Exploit learned values\n",
        "\n",
        "        next_state, reward, done, info = env.step(action) \n",
        "        \n",
        "        old_value = q_table[state, action]\n",
        "        next_max = np.max(q_table[next_state])\n",
        "        \n",
        "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
        "        q_table[state, action] = new_value\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        state = next_state\n",
        "        epochs += 1\n",
        "        \n",
        "    if i % 100 == 0:\n",
        "        clear_output(wait=True)\n",
        "        print(f\"Episode: {i}\")\n",
        "\n",
        "print(\"Training finished.\\n\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 100000\n",
            "Training finished.\n",
            "\n",
            "CPU times: user 1min 10s, sys: 15.6 s, total: 1min 25s\n",
            "Wall time: 1min 12s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxt4fmvGYBOm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d82dd51-a7ea-4633-9a60-f308b37313d4"
      },
      "source": [
        "q_table[328]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ -2.41113754,  -2.27325184,  -2.41179249,  -2.36119457,\n",
              "       -11.03601551, -10.5252337 ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6W9JE9yOYGgP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b481fa8-59cf-4700-e0d7-24f98b1f84c8"
      },
      "source": [
        "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
        "\n",
        "total_epochs, total_penalties,rewards_ep = 0,0,0,0\n",
        "episodes = 100\n",
        "\n",
        "for _ in range(episodes):\n",
        "    state = env.reset()\n",
        "    epochs, penalties, reward = 0, 0, 0\n",
        "    \n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        action = np.argmax(q_table[state])\n",
        "        state, reward, done, info = env.step(action)\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "        \n",
        "        rewards_ep+=reward\n",
        "        epochs += 1\n",
        "\n",
        "    total_penalties += penalties\n",
        "    total_epochs += epochs\n",
        "\n",
        "print(f\"Results after {episodes} episodes:\")\n",
        "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
        "print(f\"Average penalties per episode: {total_penalties / episodes}\")\n",
        "print(f\"Average reward per move: {rewards_ep/total_epochs}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 13.07\n",
            "Average penalties per episode: 0.0\n",
            "Average reward per move: 0.6067329762815609\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzzWIXGo_f4c",
        "outputId": "fc4af212-a06a-4309-a24f-5b118efd692a"
      },
      "source": [
        "total_epochs, total_penalties,rewards_ep = 0,0,0\n",
        "\n",
        "for _ in range(episodes):\n",
        "  env.s = 328  # set environment to illustration's state\n",
        "\n",
        "  epochs = 0\n",
        "  penalties, reward = 0, 0\n",
        "\n",
        "  frames = [] # for animation\n",
        "\n",
        "  done = False\n",
        "\n",
        "  while not done:\n",
        "      action = env.action_space.sample()\n",
        "      state, reward, done, info = env.step(action)\n",
        "\n",
        "      if reward == -10:\n",
        "          penalties += 1\n",
        "      \n",
        "      # Put each rendered frame into dict for animation\n",
        "      frames.append({\n",
        "          'frame': env.render(mode='ansi'),\n",
        "          'state': state,\n",
        "          'action': action,\n",
        "          'reward': reward\n",
        "          }\n",
        "      )\n",
        "      rewards_ep+=reward\n",
        "      epochs += 1\n",
        "    \n",
        "  total_penalties += penalties\n",
        "  total_epochs += epochs\n",
        "      \n",
        "      \n",
        "print(f\"Results after {episodes} episodes:\")\n",
        "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
        "print(f\"Average penalties per episode: {total_penalties / episodes}\")\n",
        "print(f\"Average reward per move: {rewards_ep/total_epochs}\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 2003.02\n",
            "Average penalties per episode: 644.53\n",
            "Average reward per move: -3.8855278529420576\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5R7gW1nLj-qE"
      },
      "source": [
        "<b><p>4. Συγκρίνετε τους δύο αλγορίθμους με βάση τις παρακάτω μετρικές</p>\n",
        "<ul>\n",
        "<li>Μέσος αριθμός παραβάσεων ανά επεισόδιο</li>\n",
        "<li>Μέσος αριθμός βημάτων ανά διαδρομή</li>\n",
        "<li>Μέσος αριθμός ανταμοιβών ανά κίνηση</li>\n",
        "</ul>\n",
        "<p>Τις παραπάνω συγκρίσεις να τις κάνετε για 100 επεισόδια.</p>\n",
        "</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODbLISC_AVlX"
      },
      "source": [
        "Μέσος αριθμός παραβάσεων ανά επεισόδιο:\n",
        "\n",
        "*   With Q learning : 0\n",
        "*   Without Q learning : 644.53\n",
        "\n",
        "Μέσος αριθμός βημάτων ανά διαδρομή:\n",
        "\n",
        "*   With Q learning : 13.07\n",
        "*   Without Q learning : 2003.02\n",
        "\n",
        "Μέσος αριθμός ανταμοιβών ανά κίνηση:\n",
        "\n",
        "*   With Q learning : 0.6067329762815609\n",
        "*   Without Q learning : -3.8855278529420576\n",
        "\n",
        "\n",
        "\n",
        "Παρατηρούμε πόσο υπερτερεί ο Q learning\n"
      ]
    }
  ]
}